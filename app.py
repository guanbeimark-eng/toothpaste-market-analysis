# -*- coding: utf-8 -*-
"""
Streamlit App: å¤šå·¥ä½œè¡¨ï¼ˆExcelï¼‰è‡ªåŠ¨è¯»å– + å¸‚åœº/äº§å“å¼€å‘åˆ†æï¼ˆä¸­æ–‡ï¼‰
- è‡ªåŠ¨éå† Excel æ‰€æœ‰ Sheet
- æ¯ä¸ª Sheet è‡ªåŠ¨è¯†åˆ«å­—æ®µï¼ˆä»·æ ¼/æ ‡é¢˜/å“ç‰Œ/è¯„åˆ†/è¯„è®ºæ•°/è§„æ ¼/è£…æ•°ç­‰ï¼‰
- å¼ºå¥ä»·æ ¼è§£æ + è¯Šæ–­
- å……è¶³å¯è§†åŒ–ï¼ˆä»·æ ¼/å•ä½ä»·/éœ€æ±‚ç»“æ„/å“ç‰Œé›†ä¸­åº¦/æ ‡ç­¾çƒ­åº¦/æœºä¼šç‚¹/ç›¸å…³æ€§ç­‰ï¼‰
- æ”¯æŒå¯¼å‡ºï¼šæ¯ä¸ªSheetä¸€å¥—åˆ†æç»“æœï¼ˆå¤šSheet Excel æŠ¥å‘Šï¼‰
"""

import re
import io
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

# =============================================================================
# 1) é¡µé¢é…ç½®
# =============================================================================
st.set_page_config(page_title="å¤šSheetå¸‚åœº&äº§å“å¼€å‘åˆ†æï¼ˆExcel/CSVï¼‰", layout="wide")
st.title("ğŸ§  å¤šå·¥ä½œè¡¨ï¼ˆExcelï¼‰â†’ å¸‚åœºæ•°æ® & äº§å“å¼€å‘æœºä¼šç‚¹åˆ†æï¼ˆä¸­æ–‡ï¼‰")
st.markdown("""
**ä½ ç°åœ¨ä¸Šä¼ çš„ Excel æœ‰å¤šä¸ªå·¥ä½œè¡¨**ï¼šæœ¬ç¨‹åºä¼š **è‡ªåŠ¨è¯»å–å¹¶é€ä¸ª Sheet åˆ†æ**ï¼Œå¹¶åœ¨æ¯ä¸ª Sheet ä¸‹è¾“å‡ºï¼š
- **ä»·æ ¼/å•ä½ä»·æ ¼/è£…æ•°/è§„æ ¼**çš„æ ‡å‡†åŒ–
- **æ ‡é¢˜æ ‡ç­¾ï¼ˆåŠŸæ•ˆ/æŠ€æœ¯/äººç¾¤/åœºæ™¯ï¼‰**æŠ½å–
- **å“ç‰Œé›†ä¸­åº¦ï¼ˆCR3/CR5/CR10ï¼‰**
- **æœºä¼šç‚¹ï¼ˆä½ä¾›ç»™é«˜éœ€æ±‚ / å•ä½ä»·æ ¼ç©ºæ¡£ / é£é™©æ ‡ç­¾ï¼‰**
- **æ›´ä¸°å¯Œçš„å¯è§†åŒ–å›¾è¡¨**
- **ä¸€é”®å¯¼å‡º Excel å¤šSheetæŠ¥å‘Š**
""")

# =============================================================================
# 2) æ–‡ä»¶åŠ è½½
# =============================================================================
def load_file(uploaded_file):
    if uploaded_file is None:
        return None, None, "æ²¡æœ‰æ–‡ä»¶"

    file_name = uploaded_file.name.lower()

    if file_name.endswith(".xlsx"):
        try:
            xl = pd.ExcelFile(uploaded_file)
            return "xlsx", xl, None
        except Exception as e:
            return None, None, f"Excel è¯»å–å¤±è´¥: {str(e)}"

    if file_name.endswith(".csv"):
        encodings = ["utf-8", "gbk", "utf-8-sig", "ISO-8859-1"]
        for enc in encodings:
            try:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=enc)
                df.columns = df.columns.astype(str).str.strip()
                return "csv", df, None
            except UnicodeDecodeError:
                continue
            except Exception as e:
                return None, None, str(e)
        return None, None, "CSV ç¼–ç è¯†åˆ«å¤±è´¥ï¼Œè¯·è½¬å­˜ä¸º UTF-8 æ ¼å¼ã€‚"

    return None, None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  .csv æˆ– .xlsx"


# =============================================================================
# 3) å¼ºå¥æ•°å€¼/ä»·æ ¼è§£æ
# =============================================================================
def clean_numeric(val):
    """
    æ›´å¼ºå¥çš„æ•°å­—æå–ï¼š
    - '$12.99', 'US$ 12.99', '12.99-18.99', '$12.99 ($2 coupon)', '12.99/Count'
    - å¯¹ '12.99-18.99' å–åŒºé—´ä¸­ä½æ•°ï¼ˆä¹Ÿå¯æ”¹æˆå–æœ€å°å€¼ï¼‰
    """
    if pd.isna(val):
        return np.nan
    if isinstance(val, (int, float, np.number)):
        return float(val)

    s = str(val).strip()
    if s == "":
        return np.nan

    s = s.replace("ï¼Œ", ",").replace("âˆ’", "-").replace("â€”", "-").replace("â€“", "-")
    s = s.replace("US$", "$").replace("USD", "$")

    nums = re.findall(r"\d+(?:\.\d+)?", s.replace(",", ""))
    if not nums:
        return np.nan

    if "-" in s or " to " in s.lower():
        if len(nums) >= 2:
            a, b = float(nums[0]), float(nums[1])
            return (a + b) / 2.0

    return float(nums[0])


# =============================================================================
# 4) åˆ—åè‡ªåŠ¨è¯†åˆ«ï¼šå…³é”®è¯è¯åº“ + åŠ æƒæ‰“åˆ†
# =============================================================================
FIELD_KEYWORDS = {
    "asin": {
        "include": ["asin", "sku", "parent asin", "child asin", "asinç ", "çˆ¶asin", "å­asin", "å•†å“id", "äº§å“id", "listing id", "item id"],
        "exclude": ["brand", "title", "name"]
    },
    "brand": {
        "include": ["brand", "å“ç‰Œ", "å“ç‰Œå", "manufacturer", "maker", "å‚ç‰Œ", "company"],
        "exclude": ["brand registry", "brand story", "title"]
    },
    "title": {
        "include": ["title", "name", "product name", "å•†å“å", "å•†å“æ ‡é¢˜", "æ ‡é¢˜", "å“å", "listing title", "product title"],
        "exclude": ["brand", "asin", "sku"]
    },
    "price": {
        "include": ["price", "å”®ä»·", "å½“å‰ä»·", "ç°ä»·", "sale price", "our price", "buy box", "buybox", "ä»·æ ¼", "current price", "amazon price"],
        "exclude": ["list price", "msrp", "coupon", "discount", "save", "off", "promo", "rebate"]
    },
    "rating": {
        "include": ["rating", "stars", "star", "è¯„åˆ†", "æ˜Ÿçº§", "average rating", "avg rating", "rating score"],
        "exclude": ["ratings count", "review", "reviews", "total ratings", "review count"]
    },
    "reviews": {
        "include": ["reviews", "review", "review count", "ratings count", "total ratings", "è¯„è®ºæ•°", "è¯„ä»·æ•°", "è¯„åˆ†æ•°", "review#", "ratings#", "number of reviews"],
        "exclude": ["rating", "stars", "star", "avg rating"]
    },
    "sales": {
        "include": ["sales", "units", "é”€é‡", "é”€å”®é‡", "unit sold", "sold", "orders", "è®¢å•é‡", "units sold"],
        "exclude": ["sales rank", "rank", "bsr"]
    },
    "revenue": {
        "include": ["revenue", "sales revenue", "é”€å”®é¢", "æˆäº¤é¢", "gmv", "é‡‘é¢", "sales $", "gross sales"],
        "exclude": ["profit", "margin", "net"]
    },
    "size": {
        "include": ["size", "net", "net content", "net wt", "net weight", "å‡€å«é‡", "å‡€é‡", "å«é‡", "oz", "ounce", "ml", "g", "gram", "volume"],
        "exclude": ["dimension", "dimensions", "length", "width", "height", "package", "shipping"]
    },
    "pack": {
        "include": ["pack", "pack of", "count", "qty", "quantity", "æ•°é‡", "è£…", "å¥—è£…", "ç»„åˆ", "variant", "variations", "variation", "flavor", "å£å‘³", "è§„æ ¼"],
        "exclude": ["package weight", "package dimensions", "shipping"]
    },
    "weight": {
        "include": ["weight", "é‡é‡", "package weight", "item weight", "shipping weight", "lbs", "lb", "pounds", "kg"],
        "exclude": ["net wt", "net weight", "å‡€é‡", "å‡€å«é‡"]
    },
    "dimensions": {
        "include": ["dimensions", "dimension", "å°ºå¯¸", "package dimensions", "item dimensions", "length", "width", "height", "cm", "inch", "inches"],
        "exclude": ["size", "net", "oz", "ml", "g", "gram"]
    },
    "rank": {
        "include": ["rank", "bsr", "best sellers rank", "æ’å", "æœç´¢æ’å", "organic rank", "ads rank", "position", "ranking"],
        "exclude": ["rating", "reviews"]
    }
}

def _norm(s: str) -> str:
    s = str(s).strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def best_column(options, field_key):
    """
    è¿”å›ï¼šæœ€ä½³åˆ—åï¼ˆstr æˆ– Noneï¼‰ä¸å¾—åˆ†ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    rules = FIELD_KEYWORDS.get(field_key)
    if rules is None or not options:
        return None, -1e9

    best_col, best_score = None, -1e9
    for col in options:
        c = _norm(col)
        score = 0

        for j, kw in enumerate(rules["include"]):
            kw_n = _norm(kw)
            if kw_n in c:
                score += 10 - min(j, 8)

        for kw in rules.get("exclude", []):
            if _norm(kw) in c:
                score -= 12

        for kw in rules["include"][:6]:
            kw_n = _norm(kw)
            if kw_n in c and len(c) <= max(len(kw_n) + 8, 18):
                score += 2

        if score > best_score:
            best_score = score
            best_col = col

    # åˆ†æ•°è¿‡ä½æ—¶è§†ä¸ºæ²¡æ‰¾åˆ°
    if best_score <= 0:
        return None, best_score
    return best_col, best_score


# =============================================================================
# 5) ç‰¹å¾å·¥ç¨‹ï¼šå‡€å«é‡ / Pack / æ ‡ç­¾
# =============================================================================
UNIT_TO_G = {
    "g": 1.0, "gram": 1.0, "grams": 1.0,
    "kg": 1000.0,
    "oz": 28.3495, "ounce": 28.3495, "ounces": 28.3495,
    "ml": 1.0,    # è¿‘ä¼¼ï¼š1mlâ‰ˆ1gï¼ˆå¦‚éœ€æ›´ç²¾ç¡®å¯æŒ‰å“ç±»å¯†åº¦è°ƒæ•´ï¼‰
    "l": 1000.0,
}

def parse_net_content_to_g(text):
    if pd.isna(text):
        return np.nan
    s = str(text).lower()
    m = re.search(r"(\d+(?:\.\d+)?)\s*(g|gram|grams|kg|oz|ounce|ounces|ml|l)\b", s)
    if not m:
        return np.nan
    val = float(m.group(1))
    unit = m.group(2)
    return val * UNIT_TO_G.get(unit, np.nan)

def parse_pack_count(text):
    if pd.isna(text):
        return 1
    s = str(text).lower()

    m = re.search(r"pack\s*of\s*(\d+)", s)
    if m:
        return int(m.group(1))

    m = re.search(r"(\d+)\s*pack\b", s)
    if m:
        return int(m.group(1))

    m = re.search(r"[xÃ—]\s*(\d+)", s)
    if m:
        return int(m.group(1))

    return 1

DEFAULT_TAG_DICT = {
    "åŠŸæ•ˆ": [
        "whitening", "brighten", "sensitivity", "sensitive", "repair", "remineral", "remineralization", "enamel",
        "gum", "fresh", "breath", "cavity", "anti-caries", "plaque", "tartar", "stain"
    ],
    "æŠ€æœ¯": [
        "nano", "hydroxyapatite", "hap", "fluoride-free", "fluoride free", "xylitol",
        "activated charcoal", "charcoal", "probiotic", "biomimetic"
    ],
    "äººç¾¤": [
        "kids", "children", "adult", "seniors", "pregnant", "braces", "orthodontic"
    ],
    "åœºæ™¯": [
        "night", "daily", "travel", "morning", "after meals"
    ]
}

def extract_tags(title, tag_dict=DEFAULT_TAG_DICT):
    if pd.isna(title):
        return {k: [] for k in tag_dict.keys()}
    t = str(title).lower()
    res = {}
    for group, kws in tag_dict.items():
        hits = []
        for kw in kws:
            if kw.lower() in t:
                hits.append(kw.lower())
        res[group] = hits
    return res


# =============================================================================
# 6) å¸‚åœºç»“æ„/æœºä¼šç‚¹
# =============================================================================
def add_price_bands(df, price_col, unit_price_col):
    df["ä»·æ ¼å¸¦"] = pd.cut(
        df[price_col],
        bins=[-0.01, 10, 15, 20, 30, 999999],
        labels=["<10", "10-15", "15-20", "20-30", "30+"]
    )
    df["å•ä½ä»·æ ¼åˆ†ä½å¸¦"] = pd.qcut(
        df[unit_price_col].replace([np.inf, -np.inf], np.nan),
        q=5,
        duplicates="drop"
    )
    return df

def brand_concentration(df, brand_col, demand_col):
    tmp = df[[brand_col, demand_col]].copy().dropna(subset=[brand_col])
    tmp[demand_col] = tmp[demand_col].fillna(0)

    brand_sum = tmp.groupby(brand_col)[demand_col].sum().sort_values(ascending=False)
    total = brand_sum.sum() if brand_sum.sum() > 0 else 1.0

    def cr(n):
        return float(brand_sum.head(n).sum() / total)

    return {
        "CR3": cr(3),
        "CR5": cr(5),
        "CR10": cr(10),
        "TopBrands": brand_sum.head(20)
    }

def tag_performance(df, tag_col, price_col, rating_col, demand_col):
    res = []
    tags = df[tag_col].dropna().unique()
    for tag in sorted(tags):
        sub = df[df[tag_col] == tag]
        res.append({
            "æ ‡ç­¾": tag,
            "SKUæ•°": len(sub),
            "è¦†ç›–ç‡": len(sub) / max(len(df), 1),
            "å‡ä»·": sub[price_col].mean(),
            "å‡è¯„åˆ†": sub[rating_col].mean(),
            "éœ€æ±‚ä»£ç†å‡å€¼": sub[demand_col].mean(),
            "éœ€æ±‚ä»£ç†æ€»é‡": sub[demand_col].sum(),
        })
    out = pd.DataFrame(res)
    if len(out) == 0:
        return out
    return out.sort_values(["éœ€æ±‚ä»£ç†æ€»é‡", "å‡è¯„åˆ†"], ascending=False)

def find_opportunities(df, price_col, unit_price_col, rating_col, demand_col, tech_tag_col, eff_tag_col):
    out = {}

    combo = df[[tech_tag_col, eff_tag_col, demand_col, rating_col, price_col, unit_price_col]].copy()
    combo = combo.dropna(subset=[tech_tag_col, eff_tag_col])
    combo["ç»„åˆ"] = combo[tech_tag_col].astype(str) + " + " + combo[eff_tag_col].astype(str)

    gp = combo.groupby("ç»„åˆ").agg(
        SKUæ•°=("ç»„åˆ", "size"),
        éœ€æ±‚ä»£ç†å‡å€¼=(demand_col, "mean"),
        éœ€æ±‚ä»£ç†æ€»é‡=(demand_col, "sum"),
        å‡è¯„åˆ†=(rating_col, "mean"),
        å‡ä»·=(price_col, "mean"),
        å•ä½ä»·æ ¼å‡å€¼=(unit_price_col, "mean")
    ).reset_index()

    if len(gp) > 0:
        gp["æœºä¼šåˆ†"] = gp["éœ€æ±‚ä»£ç†å‡å€¼"].rank(pct=True) * (1 - gp["SKUæ•°"].rank(pct=True))
        out["ä½ä¾›ç»™é«˜éœ€æ±‚"] = gp.sort_values("æœºä¼šåˆ†", ascending=False).head(15)
    else:
        out["ä½ä¾›ç»™é«˜éœ€æ±‚"] = pd.DataFrame()

    valid = df[[unit_price_col, rating_col, demand_col]].replace([np.inf, -np.inf], np.nan).dropna()
    if len(valid) > 30:
        valid = valid.copy()
        valid["å•ä½ä»·æ ¼æ¡¶"] = pd.qcut(valid[unit_price_col], q=10, duplicates="drop")
        bins = valid.groupby("å•ä½ä»·æ ¼æ¡¶").agg(
            æ¡¶å†…SKUæ•°=(unit_price_col, "size"),
            æ¡¶å†…å‡è¯„åˆ†=(rating_col, "mean"),
            æ¡¶å†…éœ€æ±‚ä»£ç†å‡å€¼=(demand_col, "mean"),
            å•ä½ä»·æ ¼æœ€å°=(unit_price_col, "min"),
            å•ä½ä»·æ ¼æœ€å¤§=(unit_price_col, "max")
        ).reset_index()

        bins["ç©ºæ¡£åˆ†"] = (1 - bins["æ¡¶å†…SKUæ•°"].rank(pct=True)) * bins["æ¡¶å†…å‡è¯„åˆ†"].rank(pct=True)
        out["ä»·æ ¼ç©ºæ¡£"] = bins.sort_values("ç©ºæ¡£åˆ†", ascending=False).head(10)
    else:
        out["ä»·æ ¼ç©ºæ¡£"] = pd.DataFrame()

    risk = df[[eff_tag_col, rating_col, demand_col]].dropna(subset=[eff_tag_col])
    rg = risk.groupby(eff_tag_col).agg(
        SKUæ•°=(eff_tag_col, "size"),
        å‡è¯„åˆ†=(rating_col, "mean"),
        éœ€æ±‚ä»£ç†æ€»é‡=(demand_col, "sum")
    ).reset_index()

    if len(rg) > 0:
        rg["é£é™©åˆ†"] = rg["SKUæ•°"].rank(pct=True) * (1 - rg["å‡è¯„åˆ†"].rank(pct=True))
        out["é£é™©ç‚¹"] = rg.sort_values("é£é™©åˆ†", ascending=False).head(15)
    else:
        out["é£é™©ç‚¹"] = pd.DataFrame()

    return out

def to_excel_bytes(sheets: dict):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        for name, sdf in sheets.items():
            if sdf is None:
                continue
            if isinstance(sdf, pd.Series):
                sdf = sdf.reset_index()
            # sheet name max 31
            sdf.to_excel(writer, sheet_name=name[:31], index=False)
    output.seek(0)
    return output


# =============================================================================
# 7) å•ä¸ªSheetçš„å®Œæ•´åˆ†æå‡½æ•°ï¼ˆè¿”å›æ¸…æ´—æ•°æ®+å¯¼å‡ºsheetå­—å…¸ï¼‰
# =============================================================================
def analyze_one_sheet(df: pd.DataFrame, sheet_name: str, allow_override: bool = True):
    df = df.copy()
    df.columns = df.columns.astype(str).str.strip()
    cols = df.columns.tolist()

    # --- è‡ªåŠ¨è¯†åˆ«åˆ— ---
    auto = {}
    scores = {}
    for k in ["asin", "brand", "title", "price", "rating", "reviews", "size", "pack", "weight", "dimensions", "sales", "revenue", "rank"]:
        c, sc = best_column(cols, k)
        auto[k] = c
        scores[k] = sc

    # --- å¯é€‰ï¼šç»™ç”¨æˆ·æ‰‹åŠ¨è¦†ç›–ï¼ˆæ¯ä¸ªsheetéƒ½èƒ½è°ƒæ•´ï¼‰ ---
    chosen = auto.copy()

    if allow_override:
        with st.expander(f"âš™ï¸ å­—æ®µæ˜ å°„ï¼ˆ{sheet_name}ï¼‰- å¯æ‰‹åŠ¨è°ƒæ•´", expanded=False):
            st.caption("ç³»ç»Ÿå·²è‡ªåŠ¨è¯†åˆ«å­—æ®µï¼›å¦‚æœè¯†åˆ«ä¸å‡†ï¼ˆå°¤å…¶æ˜¯ä»·æ ¼åˆ—ï¼‰ï¼Œè¯·åœ¨è¿™é‡Œæ”¹æ­£ç¡®ã€‚")
            c1, c2, c3, c4 = st.columns(4)
            chosen["brand"] = c1.selectbox("å“ç‰Œï¼ˆBrandï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["brand"]) if auto["brand"] in cols else 0, key=f"{sheet_name}_brand")
            chosen["title"] = c2.selectbox("æ ‡é¢˜ï¼ˆTitleï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["title"]) if auto["title"] in cols else 0, key=f"{sheet_name}_title")
            chosen["price"] = c3.selectbox("ä»·æ ¼ï¼ˆPriceï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["price"]) if auto["price"] in cols else 0, key=f"{sheet_name}_price")
            chosen["reviews"] = c4.selectbox("è¯„è®ºæ•°/è¯„ä»·æ•°ï¼ˆReviewsï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["reviews"]) if auto["reviews"] in cols else 0, key=f"{sheet_name}_reviews")

            c5, c6, c7, c8 = st.columns(4)
            chosen["rating"] = c5.selectbox("è¯„åˆ†ï¼ˆRatingï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["rating"]) if auto["rating"] in cols else 0, key=f"{sheet_name}_rating")
            chosen["size"] = c6.selectbox("å‡€å«é‡/è§„æ ¼ï¼ˆSizeï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["size"]) if auto["size"] in cols else 0, key=f"{sheet_name}_size")
            chosen["pack"] = c7.selectbox("è£…æ•°/å˜ä½“ï¼ˆPackï¼‰", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["pack"]) if auto["pack"] in cols else 0, key=f"{sheet_name}_pack")
            chosen["asin"] = c8.selectbox("ASIN/SKU", ["(None)"] + cols, index=(["(None)"] + cols).index(auto["asin"]) if auto["asin"] in cols else 0, key=f"{sheet_name}_asin")

    # --- å¿…è¦åˆ—æ ¡éªŒ ---
    if chosen["brand"] in [None, "(None)"] or chosen["title"] in [None, "(None)"]:
        st.warning(f"Sheetã€{sheet_name}ã€‘ç¼ºå°‘å“ç‰Œæˆ–æ ‡é¢˜åˆ—ï¼Œæ— æ³•åšæ ‡ç­¾/å“ç‰Œåˆ†æï¼›è¯·åœ¨å­—æ®µæ˜ å°„é‡Œè¡¥å…¨ã€‚")
        return None, {}

    # ä»·æ ¼åˆ—å…è®¸ç©ºï¼ˆä½†ä¼šå½±å“å¾ˆå¤šå›¾ï¼‰
    # --- æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹ ---
    data = df.copy()
    data["_sheet"] = sheet_name
    data["_å“ç‰Œ"] = data[chosen["brand"]].astype(str).str.strip()
    data["_æ ‡é¢˜"] = data[chosen["title"]].astype(str)

    if chosen["price"] not in [None, "(None)"]:
        data["_ä»·æ ¼"] = data[chosen["price"]].apply(clean_numeric)
    else:
        data["_ä»·æ ¼"] = np.nan

    if chosen["rating"] not in [None, "(None)"]:
        data["_è¯„åˆ†"] = data[chosen["rating"]].apply(clean_numeric)
    else:
        data["_è¯„åˆ†"] = np.nan

    # éœ€æ±‚ä»£ç†ï¼šä¼˜å…ˆ reviewsï¼Œå¦åˆ™ç”¨ sales/revenue/rank ç­‰å…œåº•ï¼Œæœ€åç”¨ 1/priceï¼ˆå¾ˆå¼±ï¼‰
    demand_source = "Reviews"
    if chosen["reviews"] not in [None, "(None)"]:
        data["_éœ€æ±‚ä»£ç†"] = data[chosen["reviews"]].apply(clean_numeric).fillna(0)
        demand_source = chosen["reviews"]
    else:
        # å°è¯• sales
        if chosen.get("sales") not in [None, "(None)"] and chosen.get("sales") in cols:
            data["_éœ€æ±‚ä»£ç†"] = data[chosen["sales"]].apply(clean_numeric).fillna(0)
            demand_source = chosen["sales"]
        elif chosen.get("revenue") not in [None, "(None)"] and chosen.get("revenue") in cols:
            data["_éœ€æ±‚ä»£ç†"] = data[chosen["revenue"]].apply(clean_numeric).fillna(0)
            demand_source = chosen["revenue"]
        elif chosen.get("rank") not in [None, "(None)"] and chosen.get("rank") in cols:
            # rank è¶Šå°è¶Šå¥½ï¼šç”¨ 1/rank
            r = data[chosen["rank"]].apply(clean_numeric)
            data["_éœ€æ±‚ä»£ç†"] = (1 / r.replace(0, np.nan)).fillna(0)
            demand_source = chosen["rank"]
        else:
            data["_éœ€æ±‚ä»£ç†"] = (1 / data["_ä»·æ ¼"].replace(0, np.nan)).fillna(0)
            demand_source = "1/Price(å¼±æ›¿ä»£)"

    if chosen["size"] not in [None, "(None)"]:
        data["_å‡€å«é‡_g"] = data[chosen["size"]].apply(parse_net_content_to_g)
    else:
        data["_å‡€å«é‡_g"] = np.nan

    if chosen["pack"] not in [None, "(None)"]:
        data["_è£…æ•°"] = data[chosen["pack"]].apply(parse_pack_count)
    else:
        data["_è£…æ•°"] = 1

    data["_å•ä½ä»·æ ¼"] = np.where(
        data["_å‡€å«é‡_g"].notna() & (data["_å‡€å«é‡_g"] > 0) & data["_è£…æ•°"].notna() & (data["_è£…æ•°"] > 0),
        data["_ä»·æ ¼"] / (data["_å‡€å«é‡_g"] * data["_è£…æ•°"]),
        data["_ä»·æ ¼"] / data["_è£…æ•°"].replace(0, np.nan)
    )

    # æ ‡ç­¾
    tags = data["_æ ‡é¢˜"].apply(lambda x: extract_tags(x, DEFAULT_TAG_DICT))
    data["_åŠŸæ•ˆæ ‡ç­¾"] = tags.apply(lambda d: d["åŠŸæ•ˆ"][0] if len(d["åŠŸæ•ˆ"]) else np.nan)
    data["_æŠ€æœ¯æ ‡ç­¾"] = tags.apply(lambda d: d["æŠ€æœ¯"][0] if len(d["æŠ€æœ¯"]) else np.nan)
    data["_äººç¾¤æ ‡ç­¾"] = tags.apply(lambda d: d["äººç¾¤"][0] if len(d["äººç¾¤"]) else np.nan)
    data["_åœºæ™¯æ ‡ç­¾"] = tags.apply(lambda d: d["åœºæ™¯"][0] if len(d["åœºæ™¯"]) else np.nan)

    # ä»·æ ¼å¸¦
    data = add_price_bands(data, "_ä»·æ ¼", "_å•ä½ä»·æ ¼")

    # æœºä¼šç‚¹
    opp = find_opportunities(
        data.replace([np.inf, -np.inf], np.nan),
        price_col="_ä»·æ ¼",
        unit_price_col="_å•ä½ä»·æ ¼",
        rating_col="_è¯„åˆ†",
        demand_col="_éœ€æ±‚ä»£ç†",
        tech_tag_col="_æŠ€æœ¯æ ‡ç­¾",
        eff_tag_col="_åŠŸæ•ˆæ ‡ç­¾"
    )

    # æ ‡ç­¾è¡¨ç°
    tech_perf = tag_performance(data, "_æŠ€æœ¯æ ‡ç­¾", "_ä»·æ ¼", "_è¯„åˆ†", "_éœ€æ±‚ä»£ç†")
    eff_perf = tag_performance(data, "_åŠŸæ•ˆæ ‡ç­¾", "_ä»·æ ¼", "_è¯„åˆ†", "_éœ€æ±‚ä»£ç†")

    # å“ç‰Œé›†ä¸­åº¦
    conc = brand_concentration(data, "_å“ç‰Œ", "_éœ€æ±‚ä»£ç†")
    top_brands = conc["TopBrands"].reset_index()
    top_brands.columns = ["å“ç‰Œ", "éœ€æ±‚ä»£ç†æ€»é‡"]

    # ä»·æ ¼å¸¦éœ€æ±‚
    band = data.groupby("ä»·æ ¼å¸¦", dropna=False)["_éœ€æ±‚ä»£ç†"].sum().reset_index()
    band.columns = ["ä»·æ ¼å¸¦", "éœ€æ±‚ä»£ç†æ€»é‡"]

    # è£…æ•°éœ€æ±‚
    pack_demand = data.groupby("_è£…æ•°")["_éœ€æ±‚ä»£ç†"].sum().reset_index()
    pack_demand.columns = ["è£…æ•°", "éœ€æ±‚ä»£ç†æ€»é‡"]

    # Top SKU
    top_sku = data.sort_values("_éœ€æ±‚ä»£ç†", ascending=False).head(20).copy()
    top_sku["_çŸ­æ ‡é¢˜"] = top_sku["_æ ‡é¢˜"].astype(str).str[:60] + "..."

    # --- å¯¼å‡º sheets ---
    export = {
        f"{sheet_name}_cleaned": data.replace([np.inf, -np.inf], np.nan),
        f"{sheet_name}_top_brands": top_brands,
        f"{sheet_name}_tech_perf": tech_perf,
        f"{sheet_name}_eff_perf": eff_perf,
        f"{sheet_name}_opp_low_supply": opp["ä½ä¾›ç»™é«˜éœ€æ±‚"],
        f"{sheet_name}_opp_price_gaps": opp["ä»·æ ¼ç©ºæ¡£"],
        f"{sheet_name}_risk": opp["é£é™©ç‚¹"],
        f"{sheet_name}_band": band,
        f"{sheet_name}_pack_demand": pack_demand,
        f"{sheet_name}_top_sku": top_sku[["_å“ç‰Œ","_çŸ­æ ‡é¢˜","_ä»·æ ¼","_å•ä½ä»·æ ¼","_è£…æ•°","_è¯„åˆ†","_éœ€æ±‚ä»£ç†","_åŠŸæ•ˆæ ‡ç­¾","_æŠ€æœ¯æ ‡ç­¾"]]
    }

    # sheet å†…å±•ç¤ºï¼šè¯Šæ–­ + å›¾è¡¨ + è¡¨æ ¼
    st.markdown("#### ğŸ§ª å…³é”®å­—æ®µè¯†åˆ«ç»“æœï¼ˆç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«ï¼‰")
    auto_show = pd.DataFrame(
        [{"å­—æ®µ": k, "è‡ªåŠ¨è¯†åˆ«åˆ—": (auto[k] if auto[k] else ""), "å¾—åˆ†": scores[k]} for k in ["brand","title","price","reviews","rating","size","pack","asin"]],
    )
    st.dataframe(auto_show, use_container_width=True, height=220)

    # ä»·æ ¼è¯Šæ–­
    st.markdown("#### ğŸ§ª ä»·æ ¼è¯»å–è¯Šæ–­")
    if chosen["price"] not in [None, "(None)"]:
        price_raw = df[chosen["price"]]
        price_clean = price_raw.apply(clean_numeric)
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ä»·æ ¼åˆ—éç©ºå æ¯”", f"{price_raw.notna().mean()*100:.1f}%")
        c2.metric("ä»·æ ¼æˆåŠŸè§£æå æ¯”", f"{price_clean.notna().mean()*100:.1f}%")
        c3.metric("è§£æåå‡ä»·", f"${price_clean.dropna().mean():.2f}" if price_clean.notna().any() else "N/A")
        c4.metric("éœ€æ±‚ä»£ç†æ¥æº", f"{demand_source}")

        bad = df.loc[price_clean.isna(), chosen["price"]].dropna().astype(str).head(20)
        if len(bad) > 0:
            st.warning("ä»¥ä¸‹ä¸ºä»·æ ¼è§£æå¤±è´¥æ ·æœ¬ï¼ˆå‰20æ¡ï¼‰ï¼Œé€šå¸¸è¯´æ˜è¯¥åˆ—å¹¶éçº¯ä»·æ ¼æˆ–æ ¼å¼è¾ƒç‰¹æ®Šï¼š")
            st.dataframe(bad.to_frame("è§£æå¤±è´¥æ ·æœ¬"), use_container_width=True)
        else:
            st.success("ä»·æ ¼è§£æçœ‹èµ·æ¥æ­£å¸¸ âœ…")
    else:
        st.warning("è¯¥ Sheet æœªé€‰æ‹©ä»·æ ¼åˆ—ï¼Œä»·æ ¼ç›¸å…³å›¾è¡¨å°†æ— æ³•å±•ç¤ºã€‚")
        st.info(f"å½“å‰éœ€æ±‚ä»£ç†æ¥æºï¼š{demand_source}")

    # æŒ‡æ ‡å¡ç‰‡
    st.markdown("#### ğŸ“Œ æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆï¼ˆäº§å“å¼€å‘è§†è§’ï¼‰")
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("SKU æ•°", f"{len(data):,}")
    m2.metric("å‡ä»·", f"${np.nanmean(data['_ä»·æ ¼']):.2f}" if np.isfinite(np.nanmean(data["_ä»·æ ¼"])) else "N/A")
    m3.metric("å‡è¯„åˆ†", f"{np.nanmean(data['_è¯„åˆ†']):.2f}" if np.isfinite(np.nanmean(data["_è¯„åˆ†"])) else "N/A")
    m4.metric("éœ€æ±‚ä»£ç†æ€»é‡", f"{data['_éœ€æ±‚ä»£ç†'].sum():,.0f}")

    # å›¾è¡¨åŒº
    st.markdown("#### ğŸ“Š å¯è§†åŒ–ï¼ˆäº§å“å¼€å‘è¶³å¤Ÿç”¨çš„ä¸€å¥—ï¼‰")
    cA, cB = st.columns(2)
    with cA:
        fig = px.histogram(data.dropna(subset=["_ä»·æ ¼"]), x="_ä»·æ ¼", nbins=25, title="ä»·æ ¼åˆ†å¸ƒï¼ˆSKUæ•°ï¼‰")
        st.plotly_chart(fig, use_container_width=True)
    with cB:
        tmp = data.replace([np.inf, -np.inf], np.nan).dropna(subset=["_å•ä½ä»·æ ¼"])
        fig = px.histogram(tmp, x="_å•ä½ä»·æ ¼", nbins=25, title="å•ä½ä»·æ ¼åˆ†å¸ƒï¼ˆè¿‘ä¼¼ï¼š$/g æˆ– $/mlï¼‰")
        st.plotly_chart(fig, use_container_width=True)

    # ä»·æ ¼å¸¦éœ€æ±‚
    fig = px.bar(band, x="ä»·æ ¼å¸¦", y="éœ€æ±‚ä»£ç†æ€»é‡", title="ä»·æ ¼å¸¦éœ€æ±‚åˆ†å¸ƒï¼ˆéœ€æ±‚ä»£ç†åŠ æƒï¼‰")
    st.plotly_chart(fig, use_container_width=True)

    # ä»·æ ¼ vs è¯„åˆ†ï¼ˆå¦‚æœ‰ï¼‰
    if data["_è¯„åˆ†"].notna().sum() > 0 and data["_ä»·æ ¼"].notna().sum() > 0:
        fig = px.scatter(
            data.replace([np.inf, -np.inf], np.nan).dropna(subset=["_ä»·æ ¼", "_è¯„åˆ†"]),
            x="_ä»·æ ¼", y="_è¯„åˆ†", size="_éœ€æ±‚ä»£ç†",
            hover_data=["_å“ç‰Œ","_æ ‡é¢˜","_å•ä½ä»·æ ¼","_è£…æ•°","_åŠŸæ•ˆæ ‡ç­¾","_æŠ€æœ¯æ ‡ç­¾"],
            title="ä»·æ ¼ vs è¯„åˆ†ï¼ˆç‚¹è¶Šå¤§=éœ€æ±‚ä»£ç†è¶Šå¤§ï¼‰"
        )
        st.plotly_chart(fig, use_container_width=True)

    # å•ä½ä»·æ ¼ vs éœ€æ±‚
    if data["_å•ä½ä»·æ ¼"].notna().sum() > 0:
        fig = px.scatter(
            data.replace([np.inf, -np.inf], np.nan).dropna(subset=["_å•ä½ä»·æ ¼"]),
            x="_å•ä½ä»·æ ¼", y="_éœ€æ±‚ä»£ç†",
            hover_data=["_å“ç‰Œ","_æ ‡é¢˜","_ä»·æ ¼","_è£…æ•°","_åŠŸæ•ˆæ ‡ç­¾","_æŠ€æœ¯æ ‡ç­¾"],
            title="å•ä½ä»·æ ¼ vs éœ€æ±‚ä»£ç†ï¼ˆåˆ¤æ–­å¸‚åœºåå¥½ï¼šæ€§ä»·æ¯” or é«˜ç«¯æº¢ä»·ï¼‰"
        )
        st.plotly_chart(fig, use_container_width=True)

    # è£…æ•°ä¸å•ä½ä»·/éœ€æ±‚
    if data["_è£…æ•°"].notna().sum() > 0 and data["_å•ä½ä»·æ ¼"].notna().sum() > 0:
        fig = px.box(
            data.replace([np.inf, -np.inf], np.nan).dropna(subset=["_è£…æ•°","_å•ä½ä»·æ ¼"]),
            x="_è£…æ•°", y="_å•ä½ä»·æ ¼", points="all", title="ä¸åŒè£…æ•°ï¼ˆPackï¼‰çš„å•ä½ä»·æ ¼åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰"
        )
        st.plotly_chart(fig, use_container_width=True)

        fig = px.bar(pack_demand, x="è£…æ•°", y="éœ€æ±‚ä»£ç†æ€»é‡", title="ä¸åŒè£…æ•°ï¼ˆPackï¼‰çš„éœ€æ±‚ä»£ç†æ€»é‡")
        st.plotly_chart(fig, use_container_width=True)

    # å“ç‰Œé›†ä¸­åº¦ + Topå“ç‰Œæ¡å½¢å›¾
    st.markdown("#### ğŸ¢ å“ç‰Œé›†ä¸­åº¦ï¼ˆCRï¼‰")
    c1, c2, c3 = st.columns(3)
    c1.metric("CR3ï¼ˆå‰3å“ç‰Œä»½é¢ï¼‰", f"{conc['CR3']*100:.1f}%")
    c2.metric("CR5ï¼ˆå‰5å“ç‰Œä»½é¢ï¼‰", f"{conc['CR5']*100:.1f}%")
    c3.metric("CR10ï¼ˆå‰10å“ç‰Œä»½é¢ï¼‰", f"{conc['CR10']*100:.1f}%")
    fig = px.bar(top_brands.head(15), x="éœ€æ±‚ä»£ç†æ€»é‡", y="å“ç‰Œ", orientation="h", title="Topå“ç‰Œï¼ˆæŒ‰éœ€æ±‚ä»£ç†æ€»é‡ï¼‰")
    st.plotly_chart(fig, use_container_width=True)

    # æ ‡ç­¾çƒ­åº¦ï¼šæŠ€æœ¯/åŠŸæ•ˆ Top15
    st.markdown("#### ğŸ·ï¸ æ ‡ç­¾çƒ­åº¦ï¼ˆç”¨äºå†³å®šäº§å“ä¸»è½´ï¼‰")
    col1, col2 = st.columns(2)
    with col1:
        tech_hot = data.groupby("_æŠ€æœ¯æ ‡ç­¾")["_éœ€æ±‚ä»£ç†"].sum().reset_index().dropna().sort_values("_éœ€æ±‚ä»£ç†", ascending=False).head(15)
        tech_hot.columns = ["æŠ€æœ¯æ ‡ç­¾", "éœ€æ±‚ä»£ç†æ€»é‡"]
        fig = px.bar(tech_hot, x="éœ€æ±‚ä»£ç†æ€»é‡", y="æŠ€æœ¯æ ‡ç­¾", orientation="h", title="æŠ€æœ¯æ ‡ç­¾çƒ­åº¦ Top15")
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        eff_hot = data.groupby("_åŠŸæ•ˆæ ‡ç­¾")["_éœ€æ±‚ä»£ç†"].sum().reset_index().dropna().sort_values("_éœ€æ±‚ä»£ç†", ascending=False).head(15)
        eff_hot.columns = ["åŠŸæ•ˆæ ‡ç­¾", "éœ€æ±‚ä»£ç†æ€»é‡"]
        fig = px.bar(eff_hot, x="éœ€æ±‚ä»£ç†æ€»é‡", y="åŠŸæ•ˆæ ‡ç­¾", orientation="h", title="åŠŸæ•ˆæ ‡ç­¾çƒ­åº¦ Top15")
        st.plotly_chart(fig, use_container_width=True)

    # Top SKUè¡¨
    st.markdown("#### ğŸ† Top SKUï¼ˆæŒ‰éœ€æ±‚ä»£ç†ï¼‰")
    st.dataframe(top_sku[["_å“ç‰Œ","_çŸ­æ ‡é¢˜","_ä»·æ ¼","_å•ä½ä»·æ ¼","_è£…æ•°","_è¯„åˆ†","_éœ€æ±‚ä»£ç†","_åŠŸæ•ˆæ ‡ç­¾","_æŠ€æœ¯æ ‡ç­¾"]],
                 use_container_width=True, height=360)

    # å“ç‰ŒÃ—ä»·æ ¼å¸¦ çƒ­åŠ›å›¾ï¼ˆTop20å“ç‰Œï¼‰
    st.markdown("#### ğŸ§Š å“ç‰Œ Ã— ä»·æ ¼å¸¦ï¼ˆéœ€æ±‚ä»£ç†çƒ­åŠ›å›¾ Top20å“ç‰Œï¼‰")
    pv = data.pivot_table(index="_å“ç‰Œ", columns="ä»·æ ¼å¸¦", values="_éœ€æ±‚ä»£ç†", aggfunc="sum", fill_value=0)
    if pv.shape[0] > 0:
        pv = pv.sort_values(pv.columns.tolist(), ascending=False).head(20)
        heat = pv.reset_index().melt(id_vars="_å“ç‰Œ", var_name="ä»·æ ¼å¸¦", value_name="éœ€æ±‚ä»£ç†")
        fig = px.density_heatmap(heat, x="ä»·æ ¼å¸¦", y="_å“ç‰Œ", z="éœ€æ±‚ä»£ç†", title="Topå“ç‰Œåœ¨ä¸åŒä»·æ ¼å¸¦çš„éœ€æ±‚åˆ†å¸ƒï¼ˆçƒ­åŠ›å›¾ï¼‰")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("å“ç‰ŒÃ—ä»·æ ¼å¸¦æ•°æ®ä¸è¶³ï¼Œçƒ­åŠ›å›¾æœªç”Ÿæˆã€‚")

    # ç›¸å…³æ€§çƒ­åŠ›å›¾
    st.markdown("#### ğŸ”— æ ¸å¿ƒæŒ‡æ ‡ç›¸å…³æ€§ï¼ˆä»·æ ¼/å•ä½ä»·/è¯„åˆ†/éœ€æ±‚ï¼‰")
    corr_cols = ["_ä»·æ ¼","_å•ä½ä»·æ ¼","_è¯„åˆ†","_éœ€æ±‚ä»£ç†"]
    corr_df = data[corr_cols].replace([np.inf, -np.inf], np.nan).dropna()
    if len(corr_df) >= 10:
        corr = corr_df.corr(numeric_only=True)
        fig = px.imshow(corr, text_auto=True, title="ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆCorrelation Heatmapï¼‰")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("å¯ç”¨äºç›¸å…³æ€§è®¡ç®—çš„æ•°æ®ä¸è¶³ï¼ˆä»·æ ¼/å•ä½ä»·/è¯„åˆ†ç¼ºå¤±è¾ƒå¤šï¼‰ã€‚")

    # æ ‡ç­¾è¡¨ç°è¡¨
    st.markdown("#### ğŸ§¾ æ ‡ç­¾è¡¨ç°ï¼ˆç”¨äºéªŒè¯æº¢ä»·/å£ç¢‘/éœ€æ±‚ï¼‰")
    t1, t2 = st.columns(2)
    with t1:
        st.markdown("**æŠ€æœ¯æ ‡ç­¾è¡¨ç°**")
        st.dataframe(tech_perf, use_container_width=True, height=360)
    with t2:
        st.markdown("**åŠŸæ•ˆæ ‡ç­¾è¡¨ç°**")
        st.dataframe(eff_perf, use_container_width=True, height=360)

    # æœºä¼šç‚¹/é£é™©ç‚¹
    st.markdown("#### ğŸ¯ æœºä¼šç‚¹ & é£é™©ç‚¹")
    o1, o2 = st.columns(2)
    with o1:
        st.markdown("**æœºä¼š1ï¼šä½ä¾›ç»™é«˜éœ€æ±‚ï¼ˆæŠ€æœ¯+åŠŸæ•ˆç»„åˆï¼‰**")
        st.dataframe(opp["ä½ä¾›ç»™é«˜éœ€æ±‚"], use_container_width=True, height=360)
    with o2:
        st.markdown("**æœºä¼š2ï¼šå•ä½ä»·æ ¼ç©ºæ¡£ï¼ˆGapï¼‰**")
        st.dataframe(opp["ä»·æ ¼ç©ºæ¡£"], use_container_width=True, height=360)

    st.markdown("**é£é™©ç‚¹ï¼šè¦†ç›–é«˜ä½†è¯„åˆ†åä½çš„åŠŸæ•ˆæ ‡ç­¾ï¼ˆé¿å‘æ¸…å•ï¼‰**")
    st.dataframe(opp["é£é™©ç‚¹"], use_container_width=True)

    # æ¸…æ´—æ•°æ®é¢„è§ˆ
    with st.expander("ğŸ” æ¸…æ´—æ•°æ®é¢„è§ˆï¼ˆå‰200è¡Œï¼‰", expanded=False):
        show_cols = ["_å“ç‰Œ","_æ ‡é¢˜","_ä»·æ ¼","_å•ä½ä»·æ ¼","_è£…æ•°","_å‡€å«é‡_g","_è¯„åˆ†","_éœ€æ±‚ä»£ç†",
                     "_åŠŸæ•ˆæ ‡ç­¾","_æŠ€æœ¯æ ‡ç­¾","_äººç¾¤æ ‡ç­¾","_åœºæ™¯æ ‡ç­¾","ä»·æ ¼å¸¦","å•ä½ä»·æ ¼åˆ†ä½å¸¦"]
        if chosen["asin"] not in [None, "(None)"]:
            data["_ASIN/SKU"] = data[chosen["asin"]].astype(str)
            show_cols = ["_ASIN/SKU"] + show_cols
        st.dataframe(data[show_cols].head(200), use_container_width=True)

    return data.replace([np.inf, -np.inf], np.nan), export


# =============================================================================
# 8) ä¸»å…¥å£ï¼šä¸Šä¼  + å¤šSheetè¯»å–ä¸åˆ†æ
# =============================================================================
st.sidebar.header("ä¸Šä¼ æ–‡ä»¶")
uploaded_file = st.sidebar.file_uploader("ä¸Šä¼  Excel(.xlsx) æˆ– CSV(.csv)", type=["xlsx", "csv"])

st.sidebar.header("åˆ†æè®¾ç½®")
allow_override = st.sidebar.checkbox("å…è®¸æ¯ä¸ªSheetæ‰‹åŠ¨è°ƒæ•´å­—æ®µæ˜ å°„ï¼ˆæ¨èå‹¾é€‰ï¼‰", value=True)
show_all_sheet_preview = st.sidebar.checkbox("å…ˆå±•ç¤ºæ‰€æœ‰Sheetçš„å‰3è¡Œé¢„è§ˆ", value=True)

if not uploaded_file:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶")
    st.stop()

file_type, data_obj, error = load_file(uploaded_file)
if error:
    st.error(error)
    st.stop()

all_sheet_exports = {}
all_cleaned = []

# CSVï¼šå½“ä½œå•sheet
if file_type == "csv":
    st.subheader("ğŸ“„ å½“å‰ä¸º CSVï¼ˆå•è¡¨ï¼‰")
    df = data_obj
    df.columns = df.columns.astype(str).str.strip()

    tab = st.tabs(["CSV_åˆ†æ"])[0]
    with tab:
        cleaned, export = analyze_one_sheet(df, "CSV", allow_override=allow_override)
        if cleaned is not None:
            all_cleaned.append(cleaned)
            all_sheet_exports.update(export)

else:
    # Excel å¤šSheet
    sheet_names = data_obj.sheet_names
    st.subheader("ğŸ“š æ£€æµ‹åˆ° Excel å¤šå·¥ä½œè¡¨")
    st.write(f"å·¥ä½œè¡¨æ•°é‡ï¼š**{len(sheet_names)}**")
    st.write(sheet_names)

    if show_all_sheet_preview:
        st.markdown("### ğŸ‘€ æ‰€æœ‰Sheetå¿«é€Ÿé¢„è§ˆï¼ˆå‰3è¡Œï¼‰")
        for sn in sheet_names:
            tmp_df = pd.read_excel(uploaded_file, sheet_name=sn)
            tmp_df.columns = tmp_df.columns.astype(str).str.strip()
            with st.expander(f"é¢„è§ˆï¼š{sn}", expanded=False):
                st.dataframe(tmp_df.head(3), use_container_width=True)

    st.markdown("### âœ… é€ä¸ªSheetåˆ†æï¼ˆæ¯ä¸ªSheetä¸€ä¸ªTabï¼‰")
    tabs = st.tabs([f"{i+1}. {sn}" for i, sn in enumerate(sheet_names)])

    for i, sn in enumerate(sheet_names):
        with tabs[i]:
            df_sheet = pd.read_excel(uploaded_file, sheet_name=sn)
            df_sheet.columns = df_sheet.columns.astype(str).str.strip()

            cleaned, export = analyze_one_sheet(df_sheet, sn, allow_override=allow_override)
            if cleaned is not None:
                all_cleaned.append(cleaned)
                all_sheet_exports.update(export)

# =============================================================================
# 9) è·¨Sheetæ±‡æ€»ï¼ˆå¦‚æœæœ‰å¤šä¸ªsheetå¯ç”¨ï¼‰
# =============================================================================
st.divider()
st.subheader("ğŸ“Œ è·¨Sheetæ±‡æ€»ï¼ˆç”¨äºè€æ¿ä¸€çœ¼çœ‹æ‡‚ï¼‰")

if len(all_cleaned) == 0:
    st.warning("æ²¡æœ‰å¯ç”¨çš„æ¸…æ´—ç»“æœï¼ˆå¯èƒ½æ‰€æœ‰Sheetéƒ½ç¼ºå°‘å“ç‰Œ/æ ‡é¢˜å­—æ®µæ˜ å°„ï¼‰ã€‚")
    st.stop()

all_data = pd.concat(all_cleaned, ignore_index=True)
# æ±‡æ€»æŒ‡æ ‡
c1, c2, c3, c4 = st.columns(4)
c1.metric("æ€»Sheetæ•°", f"{all_data['_sheet'].nunique():,}")
c2.metric("æ€»SKUæ•°", f"{len(all_data):,}")
c3.metric("å…¨è¡¨å‡ä»·", f"${np.nanmean(all_data['_ä»·æ ¼']):.2f}" if np.isfinite(np.nanmean(all_data["_ä»·æ ¼"])) else "N/A")
c4.metric("å…¨è¡¨éœ€æ±‚ä»£ç†æ€»é‡", f"{all_data['_éœ€æ±‚ä»£ç†'].sum():,.0f}")

# æ¯ä¸ªSheetï¼šä»·æ ¼è§£æç‡/å‡ä»·/éœ€æ±‚ä»£ç†
sheet_summary = []
for sn, g in all_data.groupby("_sheet"):
    price_ok = g["_ä»·æ ¼"].notna().mean() if len(g) else 0
    sheet_summary.append({
        "Sheet": sn,
        "SKUæ•°": len(g),
        "ä»·æ ¼å¯ç”¨ç‡": price_ok,
        "å‡ä»·": np.nanmean(g["_ä»·æ ¼"]),
        "å‡è¯„åˆ†": np.nanmean(g["_è¯„åˆ†"]),
        "éœ€æ±‚ä»£ç†æ€»é‡": g["_éœ€æ±‚ä»£ç†"].sum()
    })
sheet_summary_df = pd.DataFrame(sheet_summary).sort_values("éœ€æ±‚ä»£ç†æ€»é‡", ascending=False)
st.dataframe(sheet_summary_df, use_container_width=True)

# æ±‡æ€»å›¾ï¼šå„Sheetéœ€æ±‚ä»£ç†æ€»é‡
fig = px.bar(sheet_summary_df, x="éœ€æ±‚ä»£ç†æ€»é‡", y="Sheet", orientation="h", title="å„Sheetéœ€æ±‚ä»£ç†æ€»é‡ï¼ˆæ¨ªå‘å¯¹æ¯”ï¼‰")
st.plotly_chart(fig, use_container_width=True)

# =============================================================================
# 10) å¯¼å‡ºæ€»æŠ¥å‘Šï¼ˆå¤šsheet Excelï¼‰
# =============================================================================
st.divider()
st.subheader("â¬‡ï¸ å¯¼å‡ºï¼šå…¨é‡å¤šSheetåˆ†ææŠ¥å‘Šï¼ˆExcelï¼‰")

# é¢å¤–åŠ ä¸€ä¸ªâ€œæ€»æ±‡æ€»â€sheet
all_sheet_exports["0_æ€»æ±‡æ€»_sheet_summary"] = sheet_summary_df
all_sheet_exports["0_æ€»æ±‡æ€»_all_cleaned"] = all_data.replace([np.inf, -np.inf], np.nan)

excel_bytes = to_excel_bytes(all_sheet_exports)
st.download_button(
    label="ğŸ“¥ ä¸‹è½½å…¨é‡åˆ†ææŠ¥å‘Šï¼ˆExcel å¤šSheetï¼‰",
    data=excel_bytes,
    file_name="market_product_dev_multi_sheet_report_cn.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
)
